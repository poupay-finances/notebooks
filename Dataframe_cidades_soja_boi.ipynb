{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ae77d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eaedf06f-dd71-40eb-95c5-7985759d5f1c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 in central\n",
      "\tfound com.databricks#spark-xml_2.12;0.6.0 in central\n",
      "\tfound commons-io#commons-io;2.6 in central\n",
      ":: resolution report :: resolve 569ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.databricks#spark-xml_2.12;0.6.0 from central in [default]\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 from central in [default]\n",
      "\tcommons-io#commons-io;2.6 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eaedf06f-dd71-40eb-95c5-7985759d5f1c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/12ms)\n",
      "22/06/07 03:14:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#iniciar spark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "conf = SparkConf()\n",
    "#conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.2')\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.2,com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,com.databricks:spark-xml_2.12:0.6.0')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.InstanceProfileCredentialsProvider')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8470b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: S3ADRESS=s3a://daniel-3cco-poupay-poupay-bucket-estruturados\n",
      "env: PASSWORD_SQL=Urubu1@@\n",
      "env: USER_SQL=urubu100\n"
     ]
    }
   ],
   "source": [
    "%env S3ADRESS=s3a://daniel-3cco-poupay-poupay-bucket-estruturados\n",
    "%env PASSWORD_SQL=Urubu1@@\n",
    "%env USER_SQL=urubu100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fe82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "S3_address = os.environ['S3ADRESS']\n",
    "password_sql = os.environ['PASSWORD_SQL']\n",
    "user_sql = os.environ['USER_SQL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd4c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_soja = spark.read.format(\"com.databricks.spark.xml\") \\\n",
    "            .option(\"rowTag\",\"row\")\\\n",
    "            .load(f\"{S3_address}/dados-brutos/soja_xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66972cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+----+--------------------+----------+------------+---+\n",
      "| ano|             estado|index|item|                nome|quantidade|      regiao| uf|\n",
      "+----+-------------------+-----+----+--------------------+----------+------------+---+\n",
      "|2018|          São Paulo| 5287|soja|   Redenção da Serra|      0.22|     Sudeste| SP|\n",
      "|2020| Mato Grosso do Sul| 2287|soja|      Nova Andradina|      3.31|Centro-Oeste| MS|\n",
      "|2007|              Piauí| 3031|soja|     Bonfim do Piauí|      2.02|    Nordeste| PI|\n",
      "|2011|              Piauí| 3097|soja|            Itaueira|       0.5|    Nordeste| PI|\n",
      "|2019|              Goiás|  992|soja|        Damianópolis|     11.24|Centro-Oeste| GO|\n",
      "|2003|        Mato Grosso| 2406|soja|   Porto dos Gaúchos|      2.65|Centro-Oeste| MT|\n",
      "|2009|              Ceará|  634|soja|             Araripe|      0.76|    Nordeste| CE|\n",
      "|2003|           Maranhão| 1317|soja|       Primeira Cruz|      0.12|    Nordeste| MA|\n",
      "|2002|             Paraná| 3302|soja|        Congonhinhas|      0.53|         Sul| PR|\n",
      "|2003|       Minas Gerais| 1836|soja|      Mar de Espanha|      0.95|     Sudeste| MG|\n",
      "|2018|              Goiás| 1006|soja|             Formoso|      0.99|Centro-Oeste| GO|\n",
      "|2001|          São Paulo| 4865|soja|             Bálsamo|      1.16|     Sudeste| SP|\n",
      "|2004|  Rio Grande do Sul| 4129|soja|Gramado dos Loure...|       5.6|         Sul| RS|\n",
      "|2013|             Paraná| 3261|soja|     Bocaiúva do Sul|      2.38|         Sul| PR|\n",
      "|2017|Rio Grande do Norte| 3877|soja|           Vera Cruz|      2.14|    Nordeste| RN|\n",
      "|2002|         Pernambuco| 2864|soja|          Chã Grande|       1.8|    Nordeste| PE|\n",
      "|2016|              Goiás|  934|soja|             Anicuns|      1.03|Centro-Oeste| GO|\n",
      "|2008|           Amazonas|  144|soja|             Codajás|      7.06|       Norte| AM|\n",
      "|2007|           Maranhão| 1294|soja|           Paraibano|      0.67|    Nordeste| MA|\n",
      "|2000|              Goiás| 1094|soja|        Paranaiguara|      0.67|Centro-Oeste| GO|\n",
      "+----+-------------------+-----+----+--------------------+----------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_soja.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b089693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/07 03:16:27 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "22/06/07 03:16:27 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "22/06/07 03:16:27 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "_df = df_soja\n",
    "\n",
    "_df = _df.select(\n",
    "    col('ano').cast(IntegerType()).alias('ANO'),\n",
    "    col('quantidade').cast(FloatType()).alias('QUANTIDADE'),\n",
    "    col('estado').alias('ESTADO'),\n",
    "    col('uf').alias('UF'),\n",
    "    col('regiao').alias('REGIAO'),\n",
    "    col('nome').alias('CIDADE')\n",
    ")\n",
    "\n",
    "_df.write \\\n",
    "   .mode('overwrite') \\\n",
    "   .csv('s3a://daniel-3cco-poupay-poupay-bucket-estruturados/dados-tratados/cidades_producao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0781fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.csv(f\"{S3_address}/dados-tratados/cidades_producao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27990919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+---+------------+--------------------+\n",
      "| _c0|  _c1|                _c2|_c3|         _c4|                 _c5|\n",
      "+----+-----+-------------------+---+------------+--------------------+\n",
      "|2018| 0.22|          São Paulo| SP|     Sudeste|   Redenção da Serra|\n",
      "|2020| 3.31| Mato Grosso do Sul| MS|Centro-Oeste|      Nova Andradina|\n",
      "|2007| 2.02|              Piauí| PI|    Nordeste|     Bonfim do Piauí|\n",
      "|2011|  0.5|              Piauí| PI|    Nordeste|            Itaueira|\n",
      "|2019|11.24|              Goiás| GO|Centro-Oeste|        Damianópolis|\n",
      "|2003| 2.65|        Mato Grosso| MT|Centro-Oeste|   Porto dos Gaúchos|\n",
      "|2009| 0.76|              Ceará| CE|    Nordeste|             Araripe|\n",
      "|2003| 0.12|           Maranhão| MA|    Nordeste|       Primeira Cruz|\n",
      "|2002| 0.53|             Paraná| PR|         Sul|        Congonhinhas|\n",
      "|2003| 0.95|       Minas Gerais| MG|     Sudeste|      Mar de Espanha|\n",
      "|2018| 0.99|              Goiás| GO|Centro-Oeste|             Formoso|\n",
      "|2001| 1.16|          São Paulo| SP|     Sudeste|             Bálsamo|\n",
      "|2004|  5.6|  Rio Grande do Sul| RS|         Sul|Gramado dos Loure...|\n",
      "|2013| 2.38|             Paraná| PR|         Sul|     Bocaiúva do Sul|\n",
      "|2017| 2.14|Rio Grande do Norte| RN|    Nordeste|           Vera Cruz|\n",
      "|2002|  1.8|         Pernambuco| PE|    Nordeste|          Chã Grande|\n",
      "|2016| 1.03|              Goiás| GO|Centro-Oeste|             Anicuns|\n",
      "|2008| 7.06|           Amazonas| AM|       Norte|             Codajás|\n",
      "|2007| 0.67|           Maranhão| MA|    Nordeste|           Paraibano|\n",
      "|2000| 0.67|              Goiás| GO|Centro-Oeste|        Paranaiguara|\n",
      "+----+-----+-------------------+---+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d6d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumnRenamed(\"_c0\", \"ano\")\n",
    "dataframe = dataframe.withColumnRenamed(\"_c1\", \"quantidade\")\n",
    "dataframe = dataframe.withColumnRenamed(\"_c2\", \"estado\")\n",
    "dataframe = dataframe.withColumnRenamed(\"_c3\", \"uf\")\n",
    "dataframe = dataframe.withColumnRenamed(\"_c4\", \"regiao\")\n",
    "dataframe = dataframe.withColumnRenamed(\"_c5\", \"cidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c856c730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[(dataframe['regiao'] == 'Centro-Oeste') & (dataframe['ano'] == '2010')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54f57ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/07 03:23:43 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "22/06/07 03:23:43 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "22/06/07 03:23:43 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "_df = dataframe\n",
    "\n",
    "_df = _df.select(\n",
    "    col('ano').cast(IntegerType()).alias('ANO'),\n",
    "    col('quantidade').cast(FloatType()).alias('QUANTIDADE'),\n",
    "    col('estado').alias('ESTADO'),\n",
    "    col('uf').alias('UF'),\n",
    "    col('regiao').alias('REGIAO'),\n",
    "    col('cidade').alias('CIDADE')\n",
    ")\n",
    "\n",
    "_df.write \\\n",
    "   .mode('overwrite') \\\n",
    "   .csv(f'{S3_address}/dados-cliente/cidades_producao')\n",
    "\n",
    "_df.write \\\n",
    " .format('com.microsoft.sqlserver.jdbc.spark') \\\n",
    " .mode('overwrite') \\\n",
    " .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    " .option('url', 'jdbc:sqlserver://poupay-01.database.windows.net;databaseName=poupay;') \\\n",
    " .option('dbtable', 'CIDADE_PRODUTO') \\\n",
    " .option('user', user_sql) \\\n",
    " .option('password', password_sql) \\\n",
    " .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
